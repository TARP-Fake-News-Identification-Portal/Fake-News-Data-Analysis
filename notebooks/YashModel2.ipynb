{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,SpatialDropout1D,Dropout\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in /Users/yashtripathi/miniforge3/envs/nlp/lib/python3.10/site-packages (0.6.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "#import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = stopwords.words(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "### Get Data ##\n",
    "train = pd.read_csv(\"../input/train.csv\",index_col=0)\n",
    "test = pd.read_csv(\"../input/test.csv\",index_col=0)\n",
    "validation = pd.read_csv(\"../input/validation.csv\",index_col=0)\n",
    "\n",
    "# concat all\n",
    "df=test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label = pd.factorize(df['label'])[0].astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60540</th>\n",
       "      <td>nortel release delayed financial results</td>\n",
       "      <td>nortel networks said today begin releasing res...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58582</th>\n",
       "      <td>wallace fined newman collision</td>\n",
       "      <td>former nascar cup champion rusty wallace fined...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30378</th>\n",
       "      <td>north korean embassy official focus kim jong n...</td>\n",
       "      <td>kuala lumpur reuters three men wanted killing ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>trump reveals american muslim solution</td>\n",
       "      <td>home world trump reveals american muslim solut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>video watch newt gingrich unload megyn kelly t...</td>\n",
       "      <td>pinterest newt gingrich accused fox news megyn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "60540           nortel release delayed financial results   \n",
       "58582                    wallace fined newman collision    \n",
       "30378  north korean embassy official focus kim jong n...   \n",
       "2044              trump reveals american muslim solution   \n",
       "3881   video watch newt gingrich unload megyn kelly t...   \n",
       "\n",
       "                                                    text  label  \n",
       "60540  nortel networks said today begin releasing res...      0  \n",
       "58582  former nascar cup champion rusty wallace fined...      0  \n",
       "30378  kuala lumpur reuters three men wanted killing ...      0  \n",
       "2044   home world trump reveals american muslim solut...      1  \n",
       "3881   pinterest newt gingrich accused fox news megyn...      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(x, spell=spell):\n",
    "    \"\"\"correct the missplled words of a given tweet\"\"\"\n",
    "    x = x.split()\n",
    "    misspelled = spell.unknown(x)\n",
    "    result = map(lambda word : spell.correction(word) if word in  misspelled else word, x)\n",
    "    return \" \".join(result)\n",
    "\n",
    "def tweets_cleaning(x, correct_spelling=True, remove_emojis=True, remove_stop_words=True):\n",
    "    \"\"\"Apply function to a clean a tweet\"\"\"\n",
    "    x = x.lower().strip()\n",
    "    # romove urls\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    x = url.sub(r'',x)\n",
    "    # remove html tags\n",
    "    html = re.compile(r'<.*?>')\n",
    "    x = html.sub(r'',x)\n",
    "    # remove punctuation\n",
    "    operator = str.maketrans('','',string.punctuation) #????\n",
    "    x = x.translate(operator)\n",
    "    if correct_spelling:\n",
    "        x = correct_spellings(x)\n",
    "    if remove_emojis:\n",
    "        x = x.encode('ascii', 'ignore').decode('utf8').strip()\n",
    "    if remove_stop_words:\n",
    "        x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n",
    "    return x\n",
    "\n",
    "\n",
    "## APPLY the cleaning function to the text column\n",
    "df['cleaned_tweets'] = df['text'].apply(tweets_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[~df['label'].isna()]\n",
    "X_train, X_val, y_train, y_val = train_test_split(train, train['label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "# Limit on the number of features to K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences. \n",
    "# Sequences longer than this will be truncated.\n",
    "# and less than it will be padded\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, train_texts):\n",
    "        self.train_texts = train_texts\n",
    "        self.tokenizer = Tokenizer(num_words=TOP_K)\n",
    "        \n",
    "    def train_tokenize(self):\n",
    "        # Get max sequence length.\n",
    "        max_length = len(max(self.train_texts , key=len))\n",
    "        self.max_length = min(max_length, MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "        # Create vocabulary with training texts.\n",
    "        self.tokenizer.fit_on_texts(self.train_texts)\n",
    "        \n",
    "    def vectorize_input(self, tweets):\n",
    "        # Vectorize training and validation texts.\n",
    "        \n",
    "        tweets = self.tokenizer.texts_to_sequences(tweets)\n",
    "        # Fix sequence length to max value. Sequences shorter than the length are\n",
    "        # padded in the beginning and sequences longer are truncated\n",
    "        # at the beginning.\n",
    "        tweets = sequence.pad_sequences(tweets, maxlen=self.max_length, truncating='post',padding='post')\n",
    "        return tweets\n",
    "    \n",
    "tokenizer = CustomTokenizer(train_texts = X_train['text'])\n",
    "# fit o the train\n",
    "tokenizer.train_tokenize()\n",
    "tokenized_train = tokenizer.vectorize_input(X_train['text'])\n",
    "tokenized_val = tokenizer.vectorize_input(X_val['text'])\n",
    "tokenized_test = tokenizer.vectorize_input(test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import tqdm\n",
    "import requests\n",
    "import zipfile\n",
    "URL = \"http://nlp.stanford.edu/data/glove.42B.300d.zip\"\n",
    "\n",
    "def fetch_data(url=URL, target_file='../input/embedings/glove.zip', delete_zip=False):\n",
    "    #if the dataset already exists exit\n",
    "    if os.path.isfile(target_file):\n",
    "        print(\"datasets already downloded :) \")\n",
    "        return\n",
    "\n",
    "    #download (large) zip file\n",
    "    #for large https request on stream mode to avoid out of memory issues\n",
    "    #see : http://masnun.com/2016/09/18/python-using-the-requests-module-to-download-large-files-efficiently.html\n",
    "    print(\"**************************\")\n",
    "    print(\"  Downloading zip file\")\n",
    "    print(\"  >_<  Please wait >_< \")\n",
    "    print(\"**************************\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    #read chunk by chunk\n",
    "    handle = open(target_file, \"wb\")\n",
    "    for chunk in tqdm.tqdm(response.iter_content(chunk_size=512)):\n",
    "        if chunk:  \n",
    "            handle.write(chunk)\n",
    "    handle.close()  \n",
    "    print(\"  Download completed ;) :\") \n",
    "    #extract zip_file\n",
    "    zf = zipfile.ZipFile(target_file)\n",
    "    print(\"1. Extracting {} file\".format(target_file))\n",
    "    zf.extractall()\n",
    "    if delete_zip:\n",
    "        print(\"2. Deleting {} file\".format(dataset_name+\".zip\"))\n",
    "        os.remove(path=zip_file)\n",
    "\n",
    "fetch_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84141/84141 [00:00<00:00, 1063500.09it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_file = \"../input/embedings/glove.6B.300d.txt\"\n",
    "import tqdm\n",
    "\n",
    "EMBEDDING_VECTOR_LENGTH = 50 # <=200\n",
    "def construct_embedding_matrix(glove_file, word_index):\n",
    "    embedding_dict = {}\n",
    "    with open(glove_file,'r') as f:\n",
    "        for line in f:\n",
    "            values=line.split()\n",
    "            # get the word\n",
    "            word=values[0]\n",
    "            if word in word_index.keys():\n",
    "                # get the vector\n",
    "                vector = np.asarray(values[1:], 'float32')\n",
    "                embedding_dict[word] = vector\n",
    "    ###  oov words (out of vacabulary words) will be mapped to 0 vectors\n",
    "\n",
    "    num_words=len(word_index)+1\n",
    "    #initialize it to 0\n",
    "    embedding_matrix=np.zeros((num_words, EMBEDDING_VECTOR_LENGTH))\n",
    "\n",
    "    for word,i in tqdm.tqdm(word_index.items()):\n",
    "        if i < num_words:\n",
    "            vect=embedding_dict.get(word, [])\n",
    "            if len(vect)>0:\n",
    "                embedding_matrix[i] = vect[:EMBEDDING_VECTOR_LENGTH]\n",
    "    return embedding_matrix\n",
    "  \n",
    "embedding_matrix =  construct_embedding_matrix(glove_file, tokenizer.tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0s/6jmf4pkj4yd7v5smf85g9mt00000gn/T/ipykernel_14825/3535494999.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m embedding=Embedding(len(tokenizer.tokenizer.word_index)+1, # number of unique tokens\n\u001b[1;32m      4\u001b[0m                     \u001b[0mEMBEDDING_VECTOR_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#number of features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0membeddings_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# initialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "embedding=Embedding(len(tokenizer.tokenizer.word_index)+1, # number of unique tokens\n",
    "                    EMBEDDING_VECTOR_LENGTH, #number of features\n",
    "                    embeddings_initializer=Constant(embedding_matrix), # initialize \n",
    "                    input_length=MAX_SEQUENCE_LENGTH, \n",
    "                    trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "optimzer = Adam(clipvalue=0.5) # clip value to avoid the gradient exploding\n",
    "\n",
    "model.compile(optimizer=optimzer, \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0s/6jmf4pkj4yd7v5smf85g9mt00000gn/T/ipykernel_14825/3146633955.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(tokenized_train, y_train, \n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# fit the model\n",
    "history = model.fit(tokenized_train, y_train, \n",
    "\n",
    "                    batch_size=32, \n",
    "                    epochs=20, \n",
    "                    validation_data=(tokenized_val,y_val), \n",
    "                    verbose=2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "331a24a01f83adadd46afead2dc489e28f6af802c207fc8776f5eca2295cb1f5"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
